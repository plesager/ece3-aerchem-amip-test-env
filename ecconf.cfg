# Platform dependent configuration functions for the 'HPC2020' cluster
# (ECMWF, BOLOGNA)

function configure()
{
    # This function should configure all settings/modules needed to
    # later prepare the EC-Earth run directory and set variables used
    # in the run script

    # Configure paths for building/running EC-Earth
    ecearth_src_dir=/hpcperm/nm6/ecearth3/experiments/aerchemmip/sources
    run_dir=${SCRATCH}/ecearth3/${exp_name}
    ini_data_dir=/hpcperm/nm6/ece3data

    # File for standard output.
    # NOTE: This will be modified for restart jobs!
    stdout_file=${start_dir}/out/ami0.$$.out

    # Resubmit this job for automatic restarts? [true/false]
    # Also, add options for the resubmit command here.
    resubmit_job=true
    resubmit_opt=""

    # Configure grib api paths
    export ECCODES_DEFINITION_PATH=/hpcperm/nm6/ecearth3/experiments/aerchemmip/sources/util/grib_table_126:/MEMFS/definitions
    export ECCODES_SAMPLES_PATH="/MEMFS/ifs_samples/grib1"
    export GRIB_BIN_PATH=/usr/local/apps/ecmwf-toolbox/2023.04.1.0/INTEL/2021.4/bin

    # Configure number of processors per node
    proc_per_node=128

    # Configure and load modules
    pre_load_modules_cmd=""
    module_list="prgenv/intel hpcx-openmpi/2.9.0 intel-mkl/19.0.5 ecmwf-toolbox/2023.04.1.0 cdo python3/3.8.8-01"

    if [ -n "${module_list}" ]
    then
        set +u
        if [ -n "${pre_load_modules_cmd}" ]
        then
            ${pre_load_modules_cmd}
        fi
        for m in "${module_list}"
        do
            module load $m
        done
        set -u
    fi

    codes_info

    # Add directories to the shared library search path
    if [ -n "/usr/local/apps/ecmwf-toolbox/2023.04.1.0/INTEL/2021.4/lib:/usr/local/apps/hdf5-parallel/1.12.2/INTEL/2021.4/HPCX/2.9/lib:/usr/local/apps/netcdf4-parallel/4.9.1/INTEL/2021.4/HPCX/2.9/lib" ]
    then
        export LD_LIBRARY_PATH=${LD_LIBRARY_PATH:+${LD_LIBRARY_PATH}:}"/usr/local/apps/ecmwf-toolbox/2023.04.1.0/INTEL/2021.4/lib:/usr/local/apps/hdf5-parallel/1.12.2/INTEL/2021.4/HPCX/2.9/lib:/usr/local/apps/netcdf4-parallel/4.9.1/INTEL/2021.4/HPCX/2.9/lib"
    fi

    # Use machinefiles or not
    [[ `echo "$use_machinefile" | tr '[:upper:]' '[:lower:]'` == true ]] && use_machinefile=true || use_machinefile=false

    export OMP_NUM_THREADS=${SLURM_CPUS_PER_TASK:-1}

    ulimit -s unlimited
    ulimit -n 2048
    ulimit -c unlimited
    ulimit -a
}


machinefile_config()
{
    # LPJG is memory hungry (limit is somewhere between 24 and 32 cores per node)
    lpjg_exc=true
    lpjg_maxppn=30

    # # Enforced C-cycle configurations
    # if has_config ifs nemo pisces rnfmapper xios lpjg
    # then
    #     if ! has_config tm5
    #     then
    #         # c-driven
    #         ...
    #     else
    #         # e-driven
    #         ...
    #     fi
    # fi
}

machinefile_init()
{
    # Get max processes per node from the platform variable
    max_ppn=$proc_per_node

    components=( ifs nem xio rnf amip lpjg tm5 )

    # Default to sharing nodes and to using between 1 and all procs on a node
    for component in ${components[@]}
    do
        eval ${component}_exc=false
        eval ${component}_minppn=1
        eval ${component}_maxppn=$max_ppn
    done

    # Call user configuration
    machinefile_config

    # List of hosts allocated for the current job
    hosts=(`scontrol show hostname | paste -s`)
    nhosts=${#hosts[@]}
    nhosts0=$(( ${#hosts[@]} - 1 ))
    hosts[$nhosts]=dummy

    # Array to store processes as they are assigned
    declare -a -g processes_hosts
    declare -Ag has_exclusive
    for n in `seq 0 $nhosts`
    do
        processes_hosts[$n]=0
        has_exclusive[${hosts[$n]}]=false
    done

    > machinefile
}

find_empty_node()
{
    current_hostid=0
    while (( ${processes_hosts[$current_hostid]} > 0 ))
    do
        (( current_hostid += 1 ))
    done
}

find_unfilled_shared_node()
{
    current_hostid=0
    h=${hosts[$current_hostid]}
    while (( ${processes_hosts[$current_hostid]} + ${!minppn} > $max_ppn )) || ${has_exclusive[$h]}
    do
        (( current_hostid += 1 ))
        h=${hosts[$current_hostid]}
    done
}

machinefile_add()
{
    total_proc=$2

    # Iterate through all the possible binaries
    for component in ${components[@]}
    do
        binary="${component}_exe_file"
        exclusive="${component}_exc"
        minppn="${component}_minppn"
        maxppn="${component}_maxppn"

        # Check if the current binary matches the input executable
        if [ ./$(basename ${!binary}) = "$1" ]
        then

            # Allocate up to maxppn cores in each of subsequent nodes till there are no more process to assign
            while [ ${total_proc} -gt 0 ]
            do
                ${!exclusive} && find_empty_node || find_unfilled_shared_node

                [[ ${current_hostid} -gt $nhosts0 ]] && error "Not enough computing nodes"

                current_hostname=${hosts[$current_hostid]}
                nodecount=${processes_hosts[$current_hostid]}
                modelcount=0
                ${!exclusive} && has_exclusive[$current_hostname]=true

                while [[ ${total_proc} -gt 0 && $modelcount -lt ${!maxppn} && $nodecount -lt $max_ppn ]]
                do
                    echo ${hosts[$current_hostid]} >> machinefile
                    (( modelcount += 1 ))
                    (( nodecount += 1 ))
                    let "processes_hosts[$current_hostid] += 1"
                    let "total_proc -= 1" || true
                done
            done
        fi
    done
}

check_used_nodes()
{
    local id nnodes
    nnodes=0
    for id in `seq 0 $nhosts0`
    do
      (( ${processes_hosts[$id]} > 0 )) && (( nnodes+=1 ))
    done

    if (( $SLURM_NNODES > $nnodes ))
    then
        error "Too many NODES allocated, resubmit with only $nnodes nodes"
    fi
}

launch()
{
    # version using srun with or without machinefile

    if $use_machinefile
    then
        machinefile_init
        export SLURM_HOSTFILE=machinefile
    fi

    rm -f conf.txt
    _task1=-1
    NBTASKS=0
    while (( "$#" ))
    do
        nranks=$1
        executable=./$(basename $2)
        shift
        shift

        $use_machinefile && machinefile_add $executable $nranks

        _task0=$((_task1+1))
        _task1=$((_task0+nranks-1))

        cmd="${_task0}-${_task1} ${executable}"

        NBTASKS=$((NBTASKS+nranks))

        while (( "$#" )) && [ "$1" != "--" ]
        do
            cmd+=" $1"
            shift
        done
        echo ${cmd} >>conf.txt
        shift || true
    done

    echo '-------A-conf.txt------'
    cat conf.txt
    echo '-------E-conf.txt------'

    if $use_machinefile
    then
        echo '-------A-machinefile------'
        cat machinefile
        echo '-------E-machinefile------'
        check_used_nodes
    fi

    cmd="srun --kill-on-bad-exit=1"
    CONF_FILE=conf.txt
    echo "$cmd --ntasks=$NBTASKS -l --multi-prog $CONF_FILE"
    /usr/bin/time $cmd --ntasks=$NBTASKS -l --multi-prog $CONF_FILE
}

finalise()
{
    # This function should execute of any post run functionality, e.g.
    # platform dependent cleaning or a resubmit

    if ${resubmit_job} && [ $(date -d "${leg_end_date}" +%s) -lt $(date -d "${run_end_date}" +%s) ]
    then
        info "Resubmitting job for leg $((leg_number+1))"

        # Need to go to start_dir to find the run script
        cd ${start_dir}
        mkdir -p out

        unset SLURM_HOSTFILE

        # ReSubmit the same script, overwritting only the log filename.
        # Note: This does not work if you specify a job name with sbatch -J jobname!
        sbatch \
            -o out/${exp_name}.$(printf %03d $((leg_number+1))) \
            -e out/${exp_name}.$(printf %03d $((leg_number+1))) \
            ${resubmit_opt}                                     \
            ./${SLURM_JOB_NAME}
    fi

}

postprocess()
{
    local cwd=$PWD

    local islast=$(( $(date -ud "${leg_end_date}" +%s) == $(date -ud "${run_end_date}" +%s) ))
    local year1 year2 dependency mess prev_leg_nb model mods ECEMODEL

    # [1] Store LUCIA if any
    if [[ $lucia -eq -3 ]]; then
        luciabckp=$HPCPERM/ece3runs/lucia
        tstamp=$(date +%Y%m%d-%H%M%S)
        estamp=$exp_name
        has_config nemo && estamp=${estamp}"-X${xio_numproc}-N${nem_numproc}"
        has_config ifs  && estamp=${estamp}"-I${ifs_numproc}"
        has_config lpjg && estamp=${estamp}"-L${lpjg_numproc}"
        has_config tm5  && estamp=${estamp}"-T${tm5_numproc}"
        has_config amip && estamp=${estamp}"-A${amip_numproc}"
        #has_config nemo && estamp=${estamp:-}${estamp:+-}"R${rnf_numproc}"

        cp $ecearth_src_dir/oasis3-mct/util/lucia/lucia_lite.py .
        python3 lucia_lite.py minmax 10 10 40
        mv lucia_lite_plots $luciabckp/${tstamp}-$estamp-plots
        mv lucia_lite_results $luciabckp/${tstamp}-$estamp-results
        sed '1,/Hint/ d' < $luciabckp/${tstamp}-$estamp-results/summary.txt
    fi

    # Subdir for [2] and [3]
    cd $PERM/ecearth3/ece3-postproc/script
    
    # [2] Submit a hires-clim2 postprocessing job for PREVIOUS leg,
    #     and current leg if this is the last one.

    # Could replace "> 1" with "> NNN" when starting a new experiment
    # (eg scenario) as a restart (continuation) of another experiment
    # (eg historical). Else first job will fail, with no consequence.
    if (( leg_number > 1 )) || (( islast ))
    then
        year1=$(( leg_start_date_yyyy - 1 ))
        (( leg_number == 1 )) && year1=${leg_start_date_yyyy} # one-leg run case
        year2=${year1}
        (( islast )) && year2=${leg_start_date_yyyy}
        info "submit HiresClim2 job for ${year1}-${year2}"
        dependency=$(./hc.sh -6 ${exp_name} ${year1} ${year2} $(date -u -d "${run_start_date}" +%Y))
        echo $dependency
    fi

    # [3] Update timeseries every MM years or at the end of the run. MM must be > 1.
    local MM=2
    if ! (( leg_number%MM )) || (( islast ))
    then
        info "submit job to update TimeSeries at leg ${leg_number}"
        ./ts.sh -d ${dependency##* } $exp_name
        #squeue -u $USER -o '%.12i %.15j %.8u %.2t %.12M %.6D %.11a %.15E %R'
    fi

    # [4] Trigger the cmorization of the previous leg and current leg if this is the last one.
    cd /perm/nm6/ecearth3/cmor-utils/ecmwf-hpc2020
    mkdir -p log/$exp_name

    has_config ifs && mods="ifs"
    has_config nemo && mods="${mods:-}${mods:+ }nemo"
    has_config tm5 && mods="${mods:-}${mods:+ }tm5"
    has_config lpjg && mods="${mods:-}${mods:+ }lpjg"

    ECEMODEL='EC-EARTH-AerChem'

    if (( leg_number > 1 )) || (( islast ))
    then
        prev_leg_nb=$(( leg_number - 1 ))
        for model in $mods
        do
            if [ ${prev_leg_nb} -gt 0 ]
            then
                mess=${exp_name}-${model}-$(printf %03d ${prev_leg_nb})
                info "Submit cmorization $mess"
                sbatch -J $mess -o log/$exp_name/cmor_$mess.out sub-cmor.sh $exp_name $model $prev_leg_nb $output_control_files_dir ${ECEMODEL-}
            fi
            if (( islast ))
            then
                mess=${exp_name}-${model}-$(printf %03d ${leg_number})
                info "Submit cmorization $mess"
                sbatch -J $mess -o log/$exp_name/cmor_$mess.out sub-cmor.sh $exp_name $model $leg_number $output_control_files_dir ${ECEMODEL-}
            fi
        done
    fi

    cd ${cwd}
}
